{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fredrikalindh/firebase-chat-app/blob/main/rag/SpaceRAG_initial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pao9QWqE7yLn"
      },
      "outputs": [],
      "source": [
        "IN_COLAB = False\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    import os\n",
        "    os.environ[\"WANDB_API_KEY\"] = userdata.get(\"WANDB_API_KEY\")\n",
        "    os.environ[\"TOGETHER_API_KEY\"] = userdata.get(\"TOGETHER_API_KEY\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJNpOy777yLn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "repo_url = \"https://github.com/wandb/weave.git\"\n",
        "target_folder = \"weave_cookbooks\"\n",
        "subdirectory = \"examples/cookbooks\"\n",
        "branch = \"anish/add-spacerag-example\"\n",
        "\n",
        "if not os.path.exists(target_folder) and IN_COLAB:\n",
        "    print(f\"Cloning repository: {repo_url}\")\n",
        "\n",
        "    # Clone the entire repository to a temporary folder\n",
        "    temp_folder = \"temp_weave_repo\"\n",
        "    subprocess.run([\"git\", \"clone\", \"--depth\", \"1\", \"--branch\", branch, repo_url, temp_folder], check=True)\n",
        "\n",
        "    # Move the desired subdirectory to the target folder\n",
        "    shutil.move(os.path.join(temp_folder, subdirectory), target_folder)\n",
        "\n",
        "    # Remove the temporary folder\n",
        "    shutil.rmtree(temp_folder)\n",
        "\n",
        "    print(f\"Successfully cloned {subdirectory} from branch '{branch}' to {target_folder}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Folder '{target_folder}' already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPU5GWX27yLo"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(target_folder) and IN_COLAB:\n",
        "    %cd weave_cookbooks/rag/spacerag\n",
        "    !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W38uw2oh7yLr"
      },
      "outputs": [],
      "source": [
        "import weave\n",
        "from weave import Evaluation\n",
        "import os\n",
        "import numpy as np\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "from together import Together\n",
        "import re\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjF12eEJ7yLr"
      },
      "outputs": [],
      "source": [
        "weave.init('lavanyashukla/spacerag_nov4')\n",
        "\n",
        "# SERVE MODEL FROM TOGETHER ENDPOINT\n",
        "client = Together(api_key=os.environ.get(\"TOGETHER_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGbs2ryx7yLs"
      },
      "outputs": [],
      "source": [
        "# CHUNK DATA FROM EXTERNAL KNOWLEDGEBASE\n",
        "@weave.op\n",
        "def get_chunked_data(file):\n",
        "    # get data - file\n",
        "    with open(file, 'r') as file:\n",
        "        # Read the contents of the file into a variable\n",
        "        text = file.read()\n",
        "\n",
        "    # split doc into chunks\n",
        "    chunk_size = 5000 # üõ†Ô∏èLEVER\n",
        "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)] # üõ†Ô∏èLEVER (advanced)\n",
        "    return chunks\n",
        "\n",
        "# EMBED DATA\n",
        "@weave.op\n",
        "def get_text_embedding(input):\n",
        "    api_key_openai = os.environ[\"OPENAI_API_KEY\"]\n",
        "    client = OpenAI(api_key=api_key_openai)\n",
        "\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-ada-002\", # üõ†Ô∏èLEVER\n",
        "        input=input\n",
        "    )\n",
        "    return response.data[0].embedding\n",
        "\n",
        "# MAKE VECTORDB\n",
        "@weave.op\n",
        "def make_vector_db(file):\n",
        "    # get chunked data from function get_chunked_data()\n",
        "    chunks = get_chunked_data(file)\n",
        "    # embed data\n",
        "    text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n",
        "    # embed data into vectordb\n",
        "    d = text_embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(d)\n",
        "    index.add(text_embeddings)\n",
        "    return index, chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRh3mP_L7yLs"
      },
      "outputs": [],
      "source": [
        "file = './data/space.txt'\n",
        "index, chunks = make_vector_db(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8RuuvYW7yLt"
      },
      "outputs": [],
      "source": [
        "# ANSWER QUESTION\n",
        "@weave.op\n",
        "def predict(model, prompt):\n",
        "    completion = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=1.0, # üõ†Ô∏èLEVER\n",
        "        top_k=1000, # üõ†Ô∏èLEVER\n",
        "        max_tokens=1024,\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    answer = []\n",
        "    for chunk in completion:\n",
        "        if chunk.choices[0].delta.content is not None:\n",
        "            answer.append(chunk.choices[0].delta.content)\n",
        "\n",
        "    result = ''.join(answer)\n",
        "    print(result)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mLorY2M7yLt"
      },
      "outputs": [],
      "source": [
        "# RETRIEVE CHUNKS SIMILAR TO THE QUESTION\n",
        "@weave.op\n",
        "def retrieve_context(question: str) -> list:\n",
        "    question_embeddings = np.array([get_text_embedding(question)])\n",
        "    # Retrieve similar chunks from the vectorDB\n",
        "    D, I = index.search(question_embeddings, k=1) # üõ†Ô∏èLEVER k=?\n",
        "    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n",
        "    return retrieved_chunk\n",
        "\n",
        "class SpaceRAGModel(weave.Model):\n",
        "    model: str\n",
        "\n",
        "    @weave.op()\n",
        "    def predict(self, question: str):\n",
        "        retrieved_chunk = retrieve_context(question)\n",
        "        print(\"Question: \"+question)\n",
        "\n",
        "        # Combine context and question in a prompt # üõ†Ô∏èLEVER\n",
        "        prompt = f\"\"\"\n",
        "        Use this context to answer the question, don't use any prior knowledge.\n",
        "        Be concise in your answers.\n",
        "        ---------------------\n",
        "        {retrieved_chunk}\n",
        "        ---------------------\n",
        "        Question: {question}\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        answer = predict(self.model, prompt)\n",
        "        print(\"___________________________\")\n",
        "        return {'answer': answer, 'retrieved_chunk': retrieved_chunk}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpO97J2T7yLt"
      },
      "outputs": [],
      "source": [
        "def string_to_dict(input_string):\n",
        "    # Use regular expressions to find all JSON-like objects in the string\n",
        "    json_objects = re.findall(r'\\{.*?\\}', input_string)\n",
        "\n",
        "    # Initialize an empty dictionary to store the combined results\n",
        "    combined_dict = {}\n",
        "\n",
        "    for obj in json_objects:\n",
        "        try:\n",
        "            # Parse each JSON object\n",
        "            parsed_dict = json.loads(obj)\n",
        "            # Update the combined dictionary with the parsed data\n",
        "            combined_dict.update(parsed_dict)\n",
        "        except (ValueError, json.JSONDecodeError) as e:\n",
        "            print(f\"Error processing part: {obj}\\nError: {e}\")\n",
        "\n",
        "    return combined_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWA1mI3R7yLu"
      },
      "outputs": [],
      "source": [
        "dataset_ref = weave.ref(\"weave:///lavanyashukla/spacedata/object/space_dataset_llm_comprehensive:VBd5Ys7b3hGFmJJqdGTATVQYgKKrg70EiNV5FdpwFxs\").get()\n",
        "small_questions = dataset_ref.rows[:5] # üîµ NOTE: CHANGE TO 5 WHEN RUNNING OUT YOUR EXPERIMENTS TO QUICKLY TEST.\n",
        "                                        # CHANGE BACK TO 50 BEFORE FINAL LEADERBOARD SUBMISSION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ou5EOu07yLu"
      },
      "outputs": [],
      "source": [
        "def replace_nan_in_dict(result):\n",
        "    for key in result:\n",
        "        if isinstance(result[key], float) and np.isnan(result[key]):\n",
        "            result[key] = 0\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2K7y4jY7yLu"
      },
      "outputs": [],
      "source": [
        "# Evaluate with an LLM\n",
        "@weave.op\n",
        "def llm_judge_scorer(ground_truth: str, model_output: dict) -> dict:\n",
        "    scorer_llm = \"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\"\n",
        "    answer = model_output['answer']\n",
        "    retrieved_chunk = model_output['retrieved_chunk']\n",
        "\n",
        "    eval_rubrics = [\n",
        "    {\n",
        "        \"metric\": \"concise\",\n",
        "        \"rubrics\": \"\"\"\n",
        "        false: The answer is long and difficult to understand.\n",
        "        true: The answer is completely concise, readable and engaging.\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"metric\": \"relevant\",\n",
        "        \"rubrics\": \"\"\"\n",
        "        false: The answer is not relevant to the original text, or has significant flaws.\n",
        "        true: The answer is completely relevant to the original text, and provides additional value or insight.\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"metric\": \"ü•áaccurate\",\n",
        "        \"rubrics\": \"\"\"\n",
        "        Compare the factual content of the model's answer with the correct answer. Ignore any differences in style, grammar, or punctuation.\n",
        "        false: There is a disagreement between the model's answer and the correct answer.\n",
        "        true: The model's answer contains all the same details as the correct answer.\n",
        "        \"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "    scoring_prompt = \"\"\"\n",
        "    You have the correct answer, original text and the model's answer below.\n",
        "    Based on the specified evaluation metric and rubric, assign a true or false score the summary.\n",
        "    Then, return a JSON object with the metric name as the key and the evaluation score (false or true) as the value. Don't output anything else.\n",
        "\n",
        "    # Evaluation metric:\n",
        "    {metric}\n",
        "\n",
        "    # Evaluation rubrics:\n",
        "    {rubrics}\n",
        "\n",
        "    # Correct Answer\n",
        "    {ground_truth}\n",
        "\n",
        "    # Original Text\n",
        "    {retrieved_chunk}\n",
        "\n",
        "    # Model Answer\n",
        "    {model_answer}\n",
        "\n",
        "    \"\"\"\n",
        "    evals = \"\"\n",
        "    for i in eval_rubrics:\n",
        "        eval_output = predict(scorer_llm,\n",
        "            scoring_prompt.format(\n",
        "                ground_truth=ground_truth, retrieved_chunk=retrieved_chunk, model_answer=answer,\n",
        "                metric=i[\"metric\"], rubrics=i[\"rubrics\"]\n",
        "            ))+\" \"\n",
        "        evals+=eval_output\n",
        "    # evals_json = format_string_to_json(evals)\n",
        "    evals_dict = string_to_dict(evals)\n",
        "    # print(\"___________________________\")\n",
        "    # print(evals_dict)\n",
        "    # print(\"___________________________\")\n",
        "    return evals_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4cRBlJt7yLu"
      },
      "outputs": [],
      "source": [
        "# def ragas_score(question, ground_truth, model_output):\n",
        "#     from datasets import Dataset\n",
        "#     from ragas import evaluate\n",
        "#     from ragas.metrics import faithfulness, answer_relevancy, answer_correctness, context_recall, context_precision\n",
        "\n",
        "#     metric_modules = [\n",
        "#         answer_relevancy,\n",
        "#         context_recall,\n",
        "#     ]\n",
        "\n",
        "#     # Convert the retrieved_chunk to a list of strings\n",
        "#     contexts = [str(chunk) for chunk in model_output[\"retrieved_chunk\"]]\n",
        "\n",
        "#     qa_dataset = Dataset.from_dict(\n",
        "#         {\n",
        "#             \"question\": [question],\n",
        "#             \"ground_truth\": [ground_truth],\n",
        "#             \"answer\": [model_output[\"answer\"]],\n",
        "#             \"contexts\": [contexts],  # Wrap contexts in another list\n",
        "#         }\n",
        "#     )\n",
        "#     result = evaluate(qa_dataset, metrics=metric_modules,\n",
        "#                       raise_exceptions=False)\n",
        "#     return replace_nan_in_dict(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoqWszGw7yLu"
      },
      "outputs": [],
      "source": [
        "@weave.op()\n",
        "def tonic_validate_score(question: str, ground_truth: str, model_output: dict) -> dict:\n",
        "    from tonic_validate import Benchmark, ValidateScorer\n",
        "    from tonic_validate.metrics import DuplicationMetric, AnswerSimilarityMetric, AnswerConsistencyMetric\n",
        "\n",
        "    metric_modules = [DuplicationMetric(), AnswerSimilarityMetric(), AnswerConsistencyMetric()]\n",
        "\n",
        "    def get_llm_response(question):\n",
        "        return {\n",
        "            \"llm_answer\": model_output['answer'],\n",
        "            \"llm_context_list\": (\n",
        "                [model_output['retrieved_chunk']]\n",
        "                if isinstance(model_output['retrieved_chunk'], str)\n",
        "                else model_output['retrieved_chunk']\n",
        "            ),\n",
        "        }\n",
        "\n",
        "    benchmark = Benchmark(questions=[question], answers=[ground_truth])\n",
        "    scorer = ValidateScorer(metrics=metric_modules)\n",
        "    run = scorer.score(benchmark, get_llm_response)\n",
        "    return run.run_data[0].scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LnvJEcy7yLu",
        "outputId": "48abc8fb-c7d3-4e88-fb77-936bd709284d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG Model: meta-llama/Meta-Llama-3-70B-Instruct-Turbo\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How is the sense of spaciousness addressed in the design of habitats for the colony?\n",
            "The sense of spaciousness in the habitat is achieved by terracing structures up the curved walls of the torus and placing much of the commerce, light industry, and mechanical subsystems in the volume of the torus below the central plain where most inhabitants live. Walls and doors are only needed for acoustical and visual privacy, not for protection from the weather. Houses have plenty of window area to provide a sense of openness.\n",
            "___________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What are the steps involved in the process of producing aluminum from lunar soil, specifically in regards to glass grinding and acid leaching?\n",
            "There is no mention of producing aluminum from lunar soil in the provided text, nor is there any mention of glass grinding and acid leaching in the context of lunar soil processing. The text does discuss the processing of lunar ores, but it does not provide specific details on the steps involved in producing aluminum or any other specific material.\n",
            "___________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What is the importance of atmospheric pressure in large colonies in space?\n",
            "The importance of atmospheric pressure in large colonies in space is to maintain life processes adequately. The pressure must be sufficient to provide a partial pressure of oxygen (pO2) high enough for good respiration, yet low enough to avoid losses in blood cell mass and large changes in micro-organisms. A pressure too low or too high can be detrimental to human health. Additionally, the presence of an inert gas, such as nitrogen, is desirable to prevent decompression in the body's chambers and sinuses, and to provide a safety margin during accidental pressure drops or oxygen dilution.\n",
            "___________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: How does the expense of providing human workers encourage reliance on automation and the push for extreme reliability and maintainability in space commercial ventures?\n",
            "This question is not addressed in the provided context. The text discusses the challenges and considerations of establishing a human settlement in space, including the effects of ionizing radiation, weightlessness, and the need for a reliable atmosphere, but it does not mention the expense of providing human workers or the push for automation and reliability in space commercial ventures.\n",
            "___________________________\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: What are some potential benefits of long-term development in space?\n",
            "Some potential benefits of long-term development in space include:\n",
            "\n",
            "* Access to abundant solar energy\n",
            "* Utilization of resources from extraterrestrial bodies such as asteroids, Mars, and the Moon\n",
            "* Expansion of human society without harming ecosystems or displacing indigenous peoples\n",
            "* Survival of human civilization and the biosphere in case of a disaster on Earth\n",
            "* Relieving population pressure and taking industry off-Earth\n",
            "* Potential forlong-term survival and growth of humanity.\n",
            "___________________________\n",
            "{\"conscise\": true}\n",
            "{\"relevant\": true}\n",
            "{\"accurate\": true}\n",
            "{\"concise\": true}\n",
            "{\"relevant\": false}\n",
            "{\"accurate\": false}\n",
            "{\"concise\": true}\n",
            "{\"relevant\": true}\n",
            "{\"accurate\": true}\n",
            "{\"concise\": false}\n",
            "{\"relevant\": false}\n",
            "{\"accurate\": false}\n",
            "{\"concise\": true}\n",
            "{\"relevant\": true}\n",
            "{\"accurate\": true}\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 8224.13it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa54-b1a5-75d0-b471-030b143f2720\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa54-b76f-77d1-a016-989f85c82e9d\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.726000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa54-b940-7b20-a5e3-abcdb084e00d\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.065000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.185000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa54-c67e-7b53-8861-68b37e5efd69\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.033000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa55-09d6-7321-ab48-05b3f173508c\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.058000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa55-2f09-7840-8638-5693d9a60164\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Scoring responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:50<00:00, 50.76s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa55-538c-7811-a6b1-25ba2d75ff57\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1453.33it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa55-7d3b-7a43-9391-5b0ed95d0425\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa55-7f71-7e33-8801-98ba82fafedd\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 4.138000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa55-81a6-7c22-8e36-dd0e65ca3ac1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.398000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa55-8be3-71f0-8a6c-b86d287e99a6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 0.722000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa55-a085-7bd2-83a3-dc3aa3850c91\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.552000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.166000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa55-c5f8-7a62-aa47-3a2440ae51e2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.258000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 5.326000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa56-1175-70c0-b823-ddf58dda865b\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 9.970000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.420000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa56-5545-7822-8f4c-fdf957acd0cd\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 5.176000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.086000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa56-c1e7-7740-b245-bc9a5b78bbd3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 5.258000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.070000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa57-42e2-7931-9aad-8f27e10305ff\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.296000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa57-5d3e-7c23-83ac-460f0c0b9f7d\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Scoring responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [02:19<00:00, 139.65s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa57-7daa-77f3-b585-f7e0e000ce13\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrieving responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 6195.43it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa57-a456-7213-aa73-0fd0e4a0fa3e\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa57-a7c8-7a13-9bb7-c32be235db65\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.054000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa57-aa4e-70f2-878b-adbe7a0871c2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.272000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa57-b9b9-7c93-936f-33d7f55f0887\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 5.666000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa57-c621-79d1-b187-f81b8fb08a6f\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.777000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 4.410000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.758000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa58-02ba-7da2-a7da-2b4366346b8d\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.238000 seconds\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa58-37a7-71d2-9289-f76ed28d488f\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Scoring responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:55<00:00, 55.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa58-5cdc-7543-afbd-7c75e947da44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 377.02it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa58-83ff-7d71-b837-cb5f3845b8af\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa58-8671-71a2-8049-582302936f73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 2.574000 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa58-8841-72f2-8691-5f24e8b53c16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.836000 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa58-9685-7df3-9f88-c4ab1d552dfe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.586000 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa58-a4d5-7120-ad93-01391bc52693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.646000 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa58-c88d-7ff2-8d8c-77f38be24d64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.660000 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa58-eb1b-73e1-b55f-28b1846bc1e8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.782000 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa59-0de1-7d71-9d74-7519b1a5a7d0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 7.710000 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa59-3047-7d60-9b2d-cbc0ab7ba36a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "Scoring responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [01:02<00:00, 62.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa59-535b-7242-9261-72ea706e2e45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving responses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 7810.62it/s]\n",
            "Scoring responses:   0%|          | 0/1 [00:00<?, ?it/s]INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa59-7c0c-7713-ac93-3430816697b4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa59-7e33-7b52-b9e5-98e81dbde7ca\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 5.270000 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa59-80a0-7260-a5ca-8f7b6c22569e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.058000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üç© https://wandb.ai/lavanyashukla/spacerag_nov4/r/call/0192fa59-8bf5-7a31-8340-48bce38d8cca\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 6.932000 seconds\n",
            "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
            "INFO:openai._base_client:Retrying request to /chat/completions in 8.014000 seconds\n"
          ]
        }
      ],
      "source": [
        "models = [\"meta-llama/Meta-Llama-3-70B-Instruct-Turbo\",\n",
        "          \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
        "          \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
        "          ]  # üõ†Ô∏èLEVER - find more models @ https://docs.together.ai/docs/serverless-models#chat-models\n",
        "for model in models:\n",
        "    rag_model = SpaceRAGModel(model=model)\n",
        "    model_name = model.split('/')[-1]  # Get only the part after the '/'\n",
        "    evaluation = Evaluation(name=f\"spacerag-{model_name}\", dataset=small_questions, scorers=[\n",
        "    llm_judge_scorer,\n",
        "    # ragas_score,\n",
        "    tonic_validate_score\n",
        "])\n",
        "    print(f\"RAG Model: {model}\")\n",
        "    await evaluation.evaluate(rag_model, __weave={\"display_name\": f\"spacerag-{model_name}\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLGtl-vM7yLu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "name": "SpaceRAG-initial.ipynb",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}